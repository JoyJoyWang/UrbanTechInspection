## Methodology

### SAM (Segment Anything Model)
The SAM model is used to generate masks for images, enabling precise segmentation of various objects within an image. It is loaded from a pre-trained checkpoint and configured to generate masks in COCO RLE (Run-Length Encoding) format.

### SSA (Semantic Segmentation Annotation)
The SSA model is designed to provide semantic segmentation annotations, identifying and labeling objects within an image. It works in conjunction with the SAM model to enhance the accuracy of segmentation.

### ONEFORMER
ONEFORMER is a versatile model architecture trained on the COCO and ADE20k datasets. It is used for object recognition within the segmented regions (anns) generated by the SAM model. It provides lists of identified objects for both COCO and ADE20k.

### COCO (Common Objects in Context)
COCO is a large-scale object detection, segmentation, and captioning dataset. In this project, it is used to train models for identifying common objects within segmented regions of images.

### ADE20k
ADE20k is a dataset used for semantic segmentation. Similar to COCO, it is utilized to train models for identifying objects within segmented regions.

### ADK20 (Annotation Dataset Kit 20)
ADK20 refers to the custom annotation dataset used in this project. It includes specific annotations for training and evaluating the segmentation and recognition models.

### BLIP (Bootstrapped Language Image Pre-training)
BLIP is a model used for generating descriptive captions for image segments. It is pre-trained and helps in generating descriptive labels for segmented image regions.

### CLIP (Contrastive Language-Image Pre-training)
CLIP is a model used for zero-shot classification of objects within image segments. It matches object names with descriptive captions generated by BLIP to provide the top 5 most likely object names.

### CLIPSeg
CLIPSeg is used for pixel-level semantic segmentation of image segments. It assigns each pixel in a segment to a semantic category such as person, car, tree, etc.

### Unet-vgg16
Unet-vgg16 is a convolutional neural network used for crack detection in images. It evaluates images or image patches to generate probability heatmaps indicating the presence of cracks.

## File Description

### SSA-UT-model
This is the model code, primarily in Python files.

### UI2.py
A local frontend written in Python. Due to the limited functionality and aesthetics of tkinter, subsequent development was done for the web-based sst-app.

### main.py
This file configures the SAM model and calls the data processing pipeline.

#### The SAM model (Segment Anything Model)
The SAM model is used for generating masks for images. Here's how the SAM model is utilized:

**Loading the SAM Model:**

The SAM model is loaded from the specified checkpoint path when the command-line argument `--sam` is set to True. The model is registered and loaded using the `sam_model_registry` function from the `segment_anything` library, and then it is moved to the specified device (e.g., GPU or CPU).

```python
if args.sam:
    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
    sam = sam_model_registry["vit_h"](checkpoint=args.ckpt_path).to(device)
    
```

**Configuring the SAM Mask Generator:**

Depending on whether `light_mode` is enabled, the code configures the mask generator `SamAutomaticMaskGenerator` with different parameters, such as the number of points per side, prediction IoU threshold, stability score threshold, and others. The mask generator produces masks in COCO RLE (Run-Length Encoding) format. Due to my computer's resource limitations, I set the `points_per_side` in normal mode to be the same as in light mode (32). Setting it higher would provide finer segmentation but slower performance.

```python
if args.light_mode:
    mask_generator = SamAutomaticMaskGenerator(
        model=sam,
        points_per_side=32,
        pred_iou_thresh=0.86,
        stability_score_thresh=0.92,
        crop_n_layers=0,
        crop_n_points_downscale_factor=2,
        min_mask_region_area=100,
        output_mode='coco_rle'
    )
else:
    mask_generator = SamAutomaticMaskGenerator(
        model=sam,
        points_per_side=32,
        pred_iou_thresh=0.86,
        stability_score_thresh=0.92,
        crop_n_layers=0,
        crop_n_points_downscale_factor=2,
        min_mask_region_area=100,
        output_mode='coco_rle',
    )
```

**Processing Images and Generating Masks:**

The image dataset is loaded using `DataLoader`, and images are processed in batches. For each batch of images, the code calls the `semantic_annotation_pipeline` function to perform semantic annotation. The `mask_generator` (i.e., the SAM mask generator) is passed as an argument to this function.

```python
dataset = ImageDataset(args.data_dir, args.out_dir)
dataloader = DataLoader(dataset, batch_size=args.batch_size)

for batch in dataloader:
    imgs, filename = batch
    img = imgs[0]
    filename = filename[0]

    with torch.no_grad():
        semantic_annotation_pipeline(img, filename, args.data_dir, args.out_dir, device, save_img=args.save_img, save_json=args.save_json,
                                     clip_processor=clip_processor, clip_model=clip_model,
                                     oneformer_ade20k_processor=oneformer_ade20k_processor,
                                     oneformer_ade20k_model=oneformer_ade20k_model,
                                     oneformer_coco_processor=oneformer_coco_processor,
                                     oneformer_coco_model=oneformer_coco_model,
                                     clipseg_processor=clipseg_processor, clipseg_model=clipseg_model,
                                     mask_generator=mask_generator)
    gc.collect()
```

In summary, the SAM model in this script is primarily used to generate masks for images. First, it loads a pre-trained SAM model using `sam_model_registry`. Then, it configures the `SamAutomaticMaskGenerator` with appropriate parameters. Finally, during the image processing pipeline, it generates and applies the masks by passing the `mask_generator` to the `semantic_annotation_pipeline` function.

### pipeline.py

This is the main data processing and model inference file. It first uses the `mask_generator` (the SAM model) to segment the image, obtaining `anns`, which stores each segmented part. Using models trained on the COCO and ADE20K datasets, it identifies objects within each element in `anns`, generating two lists of recognition results. Each segmented block is then cropped into rectangular patches, and the pre-trained BLIP model is used to generate descriptions for each patch. The spaCy library extracts nouns from these descriptions. For each patch, a list of the top K object names from the OneFormer-generated COCO and ADE20K labels, along with nouns generated by the BLIP model, is passed to the CLIP model for zero-shot classification. The top 5 matching object names are saved as semantic segmentation results in `mask_categories`.

The CLIPSeg model is then used to perform pixel-level annotation, assigning each pixel in the patch to its semantic category (e.g., person, car, tree). The category with the most pixels is assigned as `ann['class_name']`.

Next, parts related to walls are extracted. The extraction method involves checking if the highest probability classification result (`ann['class_name']`) contains "wall", "building", or "crack", and if the top 2 results in `mask_categories` also contain these keywords. Users can set `exclude_words` through the UI, which will also serve as potential labels for the CLIP model to improve classification accuracy. During wall extraction, any parts where the top item in CLIP classification results exists in `exclude_words` are excluded from the `anns` segmentation.

### infer_crack.py

**Importing Libraries and Modules:**

Necessary Python libraries and modules are imported, including OpenCV (cv2) for image processing, PyTorch for deep learning, torchvision for data processing, and matplotlib and argparse for visualization.

**evaluate_img Function:**

The `evaluate_img` function performs crack detection on an entire image. It preprocesses and resizes the input image, predicts the crack probability distribution using the loaded UNet16 model, and returns a crack probability heatmap.

**evaluate_img_patch Function:**

The `evaluate_img_patch` function performs patch-based crack detection. It first loads a pre-generated wall mask image, matches it with the input image to generate a binary mask, then divides the input image into multiple patches, feeds them into the model for crack detection, and combines the results to generate the final crack probability heatmap.

**Visualization and Saving Functions:**

Functions such as `save_heatmap`, `save_overlay`, and `draw_boxes` generate and save visualizations of the crack detection results, including heatmaps, overlays, and images with bounding boxes.

**Main Program:**

The main program first parses command-line arguments, including image folder path, model path, and model type. It then processes each image, calls the appropriate crack detection function, generates and saves various visualizations, and classifies images into categories with and without cracks based on detection results.

### utils_crack.py

**Importing Libraries and Modules:**

Necessary Python libraries and modules are imported, including numpy for data processing, PyTorch for deep learning, and tqdm for progress display.

**Utility Functions and Classes:**

The `AverageMeter` class calculates and updates the average of metrics. The `cuda` function moves tensors to the GPU, if available. The `write_event` function records events and metrics during training. Other functions, such as `create_model` and `load_unet_vgg16`, create and load specific types of UNet models, supporting different pre-trained models.

**Training Function train:**

The `train` function handles the training process, including initializing the optimizer, defining the loss function, iterating through the training dataset, and recording training loss and validation metrics. This function supports interrupting and resuming training, as well as multi-fold cross-validation.

### sst-app

This code is for developing the website, currently focusing on frontend functionalities.

**Functionality Links [TBD]:**

To be completed. The plan is to use the Flask framework to convert the models into APIs, which can then be called within the web app.



## Installation

### 3.1\. Clone the Project

bash


```
git clone https://github.com/your_username/your_project.git
cd your_project
```

### 3.2 Create and Activate a Virtual Environment

Create a virtual environment using `venv`:

bash


```
python -m venv ssa-ut
```

Activate the virtual environment:

For Windows users:

bash


```
.\ssa-ut\Scripts\activate
```

For MacOS/Linux users:

bash


```
source ssa-ut/bin/activate
```

### 3.3\. Install Dependencies

Install dependencies in the activated virtual environment:

bash


```
pip install -r requirements.txt
```

### 3.4 Prepare Pre-trained Models

Place the image data to be processed into the `data` folder under the `SSA-UT-model` directory.

Download the SAM checkpoint and place it into the `ckp` folder:

bash


```
mkdir ckp && cd ckp
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
cd ..
```

Download the Unet-vgg16 checkpoint [here](https://drive.google.com/file/d/1wA2eAsyFZArG3Zc9OaKvnBuxSAPyDl08/view) and place it into the `ckp` folder.

### 3.5 Run

(1) Use `UI2.py` to set hyperparameters and run. (2) Or run from the command line:

bash


```
python main.py --data_dir ./data --out_dir ./output --ckpt_path ./ckp/sam_vit_h_4b8939.pth --save_img --save_json
```

In `infer_crack.py`, default hyperparameters are set. To adjust hyperparameters via command line, run:

bash


```
python infer_crack.py -img_dir ./data/images -model_path ./models/crack_detection_model.pth -model_type vgg16 -out_viz_dir ./output/visualization -out_pred_dir ./output/predictions -threshold 0.1
```